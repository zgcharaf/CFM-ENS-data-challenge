{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7977791,"sourceType":"datasetVersion","datasetId":4695158}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport polars as pl\nfrom tensorflow.keras.utils import to_categorical\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Bidirectional, GRU, Dense, Dropout\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Bidirectional, GRU, Dense, Dropout","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-11T16:50:25.957901Z","iopub.execute_input":"2024-05-11T16:50:25.958724Z","iopub.status.idle":"2024-05-11T16:50:25.965380Z","shell.execute_reply.started":"2024-05-11T16:50:25.958682Z","shell.execute_reply":"2024-05-11T16:50:25.964271Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-05-11T16:50:36.282300Z","iopub.execute_input":"2024-05-11T16:50:36.282718Z","iopub.status.idle":"2024-05-11T16:50:36.289634Z","shell.execute_reply.started":"2024-05-11T16:50:36.282675Z","shell.execute_reply":"2024-05-11T16:50:36.288573Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"/kaggle/input/cfmens/X_test_m4HAPAP.csv\n/kaggle/input/cfmens/y_train_or6m3Ta.csv\n/kaggle/input/cfmens/X_train_N1UvY30.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train = pl.scan_csv('/kaggle/input/cfmens/X_train_N1UvY30.csv')\ndef add_feature_engineering(df):\n    # Imbalance Metrics\n    df = df.with_columns([\n        (pl.col('bid_size') - pl.col('ask_size')).alias('imbalance'),\n        (pl.col('bid_size') / (pl.col('ask_size') + 0.01)).alias('imbalance_ratio')  # Avoid division by zero\n    ])\n    \n    # Recent Price Change\n    # Calculate price change within each observation id assuming each group 'obs_id' is sorted by time\n    df = df.with_columns(\n    (pl.col('price').diff().fill_null(0).cumsum().over('obs_id')).alias('cumulative_price_change')\n)\n    \n\n    # VWAP (Volume-Weighted Average Price)\n    # VWAP is calculated as the sum of (price * volume) divided by the total volume\n    price_times_volume = pl.col('price') * pl.col('bid_size')  # Use bid_size as a proxy for volume\n    total_volume = pl.col('bid_size')\n    \n    df = df.with_columns(\n        ((price_times_volume) / (total_volume)).alias('vwap')\n    )\n\n    return df\n\n# Apply feature engineering\ndf = add_feature_engineering(X_train)\norder_counts = df.groupby(['obs_id', 'order_id']).agg(pl.count().alias('count'))\ndf = df.join(order_counts, on=['obs_id', 'order_id'])\n\n# Step 2: Create shifted columns conditionally\n# Using mask to apply shift only to rows where 'count' > 1\ncolumns_to_shift = ['action', 'venue', 'side', 'bid', 'ask', 'bid_size', 'ask_size', 'trade']\nfor col in columns_to_shift:\n    df = df.with_columns(\n        pl.when(pl.col('count') > 1)\n        .then(pl.col(col).shift(1).over(['obs_id', 'order_id']))\n        .otherwise(pl.lit(None))\n        .alias(f'prev_{col}')\n    )\n\n# Step 3: Optionally, remove the 'count' column if no longer needed\ndf = df.drop('count')\ndf = df.fill_null(0)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T14:57:25.903612Z","iopub.execute_input":"2024-05-11T14:57:25.904757Z","iopub.status.idle":"2024-05-11T14:57:26.462130Z","shell.execute_reply.started":"2024-05-11T14:57:25.904719Z","shell.execute_reply":"2024-05-11T14:57:26.461123Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_2213/529232542.py:13: DeprecationWarning: `cumsum` is deprecated. It has been renamed to `cum_sum`.\n  (pl.col('price').diff().fill_null(0).cumsum().over('obs_id')).alias('cumulative_price_change')\n/tmp/ipykernel_2213/529232542.py:30: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n  order_counts = df.groupby(['obs_id', 'order_id']).agg(pl.count().alias('count'))\n/tmp/ipykernel_2213/529232542.py:30: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n  order_counts = df.groupby(['obs_id', 'order_id']).agg(pl.count().alias('count'))\n","output_type":"stream"}]},{"cell_type":"code","source":"\ncategorical_columns = ['venue', 'action', 'side','trade', 'prev_action', 'prev_venue', 'prev_trade']\n\nfor col in categorical_columns:\n    df = df.with_columns(pl.col(col).cast(pl.Utf8).cast(pl.Categorical).alias(col))\n\n# To one-hot encode, we can use `to_dummies` (similar to pandas get_dummies)\n\nX_train_p = df.collect().to_pandas()\nX_train_encoded = pd.get_dummies(X_train_p, columns=[\n    'venue', 'action', 'side', 'trade', \n    'prev_action', 'prev_venue', 'prev_trade', 'prev_side'\n], drop_first=True).drop(['obs_id'], axis=1)\n\n# List of quantitative variables\n# You should replace the placeholders with the actual names of your quantitative variables\nquant_vars = [\n \n 'order_id',\n 'price',\n 'bid',\n 'ask',\n 'bid_size',\n 'ask_size',\n 'flux',\n 'imbalance',\n 'imbalance_ratio',\n 'cumulative_price_change',\n 'vwap',\n 'prev_bid',\n 'prev_ask',\n 'prev_bid_size',\n 'prev_ask_size',\n ]\n\n# Initialize the StandardScaler\nscaler = StandardScaler()\n\n# Fit and transform the quantitative variables\nX_train_encoded[quant_vars] = scaler.fit_transform(X_train_encoded[quant_vars])\n\n# Now X_train_encoded has normalized quantitative variables and encoded categorical variables\n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T15:02:08.904135Z","iopub.execute_input":"2024-05-11T15:02:08.904584Z","iopub.status.idle":"2024-05-11T15:02:24.241035Z","shell.execute_reply.started":"2024-05-11T15:02:08.904549Z","shell.execute_reply":"2024-05-11T15:02:24.240093Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"X_train_encoded","metadata":{"execution":{"iopub.status.busy":"2024-05-11T15:02:24.242848Z","iopub.execute_input":"2024-05-11T15:02:24.243309Z","iopub.status.idle":"2024-05-11T15:02:29.226108Z","shell.execute_reply.started":"2024-05-11T15:02:24.243279Z","shell.execute_reply":"2024-05-11T15:02:29.225001Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"          order_id     price       bid       ask  bid_size  ask_size  \\\n0        -1.634780 -0.002671 -0.004684 -0.052716 -0.445946 -0.588867   \n1        -1.585247 -0.004831 -0.004684 -0.052716 -0.445946 -0.588867   \n2        -1.535713 -0.002763 -0.004684 -0.052716 -0.445946 -0.588867   \n3        -1.486179 -0.002671 -0.004684 -0.052716 -0.445946 -0.588867   \n4        -1.436645 -0.002350 -0.004684 -0.052716 -0.445946 -0.588867   \n...            ...       ...       ...       ...       ...       ...   \n16079995  1.386790  0.002016  0.268808 -0.048209  0.412520 -0.231378   \n16079996  1.832596 -0.003774  0.268808 -0.048209  0.412520 -0.093882   \n16079997  1.882130  0.001741  0.268808 -0.048209  0.412520 -0.093882   \n16079998  1.931664  0.001741  0.268808 -0.048209  0.412520 -0.093882   \n16079999  1.981197 -0.004050  0.268808 -0.048209  0.277329 -0.093882   \n\n              flux  imbalance  imbalance_ratio  cumulative_price_change  ...  \\\n0         0.686083   0.107172         0.872301                -0.001111  ...   \n1         0.686083   0.107172         0.872301                -0.003273  ...   \n2        -0.686455   0.107172         0.872301                -0.001203  ...   \n3         0.686083   0.107172         0.872301                -0.001111  ...   \n4        -0.686455   0.107172         0.872301                -0.000789  ...   \n...            ...        ...              ...                      ...  ...   \n16079995 -0.686455   0.515532        -0.150774                 0.004962  ...   \n16079996  0.686083   0.406636        -0.159070                -0.000835  ...   \n16079997  0.686083   0.406636        -0.159070                 0.004686  ...   \n16079998  0.686083   0.406636        -0.159070                 0.004686  ...   \n16079999  0.006677   0.297740        -0.162016                -0.001111  ...   \n\n          trade_true  prev_action_U  prev_action_D  prev_venue_4  \\\n0              False          False          False         False   \n1              False          False          False         False   \n2              False          False          False         False   \n3              False          False          False         False   \n4              False          False          False         False   \n...              ...            ...            ...           ...   \n16079995       False          False          False          True   \n16079996       False          False          False         False   \n16079997       False          False          False         False   \n16079998       False          False          False         False   \n16079999       False          False          False         False   \n\n          prev_venue_1  prev_venue_2  prev_venue_5  prev_venue_3  \\\n0                False         False         False         False   \n1                False         False         False         False   \n2                False         False         False         False   \n3                False         False         False         False   \n4                False         False         False         False   \n...                ...           ...           ...           ...   \n16079995         False         False         False         False   \n16079996         False         False         False         False   \n16079997         False         False         False         False   \n16079998         False         False         False         False   \n16079999         False         False         False         False   \n\n          prev_trade_true  prev_side_B  \n0                   False        False  \n1                   False        False  \n2                   False        False  \n3                   False        False  \n4                   False        False  \n...                   ...          ...  \n16079995            False        False  \n16079996            False        False  \n16079997            False        False  \n16079998            False        False  \n16079999            False        False  \n\n[16080000 rows x 33 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>order_id</th>\n      <th>price</th>\n      <th>bid</th>\n      <th>ask</th>\n      <th>bid_size</th>\n      <th>ask_size</th>\n      <th>flux</th>\n      <th>imbalance</th>\n      <th>imbalance_ratio</th>\n      <th>cumulative_price_change</th>\n      <th>...</th>\n      <th>trade_true</th>\n      <th>prev_action_U</th>\n      <th>prev_action_D</th>\n      <th>prev_venue_4</th>\n      <th>prev_venue_1</th>\n      <th>prev_venue_2</th>\n      <th>prev_venue_5</th>\n      <th>prev_venue_3</th>\n      <th>prev_trade_true</th>\n      <th>prev_side_B</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.634780</td>\n      <td>-0.002671</td>\n      <td>-0.004684</td>\n      <td>-0.052716</td>\n      <td>-0.445946</td>\n      <td>-0.588867</td>\n      <td>0.686083</td>\n      <td>0.107172</td>\n      <td>0.872301</td>\n      <td>-0.001111</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.585247</td>\n      <td>-0.004831</td>\n      <td>-0.004684</td>\n      <td>-0.052716</td>\n      <td>-0.445946</td>\n      <td>-0.588867</td>\n      <td>0.686083</td>\n      <td>0.107172</td>\n      <td>0.872301</td>\n      <td>-0.003273</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.535713</td>\n      <td>-0.002763</td>\n      <td>-0.004684</td>\n      <td>-0.052716</td>\n      <td>-0.445946</td>\n      <td>-0.588867</td>\n      <td>-0.686455</td>\n      <td>0.107172</td>\n      <td>0.872301</td>\n      <td>-0.001203</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1.486179</td>\n      <td>-0.002671</td>\n      <td>-0.004684</td>\n      <td>-0.052716</td>\n      <td>-0.445946</td>\n      <td>-0.588867</td>\n      <td>0.686083</td>\n      <td>0.107172</td>\n      <td>0.872301</td>\n      <td>-0.001111</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.436645</td>\n      <td>-0.002350</td>\n      <td>-0.004684</td>\n      <td>-0.052716</td>\n      <td>-0.445946</td>\n      <td>-0.588867</td>\n      <td>-0.686455</td>\n      <td>0.107172</td>\n      <td>0.872301</td>\n      <td>-0.000789</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16079995</th>\n      <td>1.386790</td>\n      <td>0.002016</td>\n      <td>0.268808</td>\n      <td>-0.048209</td>\n      <td>0.412520</td>\n      <td>-0.231378</td>\n      <td>-0.686455</td>\n      <td>0.515532</td>\n      <td>-0.150774</td>\n      <td>0.004962</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>16079996</th>\n      <td>1.832596</td>\n      <td>-0.003774</td>\n      <td>0.268808</td>\n      <td>-0.048209</td>\n      <td>0.412520</td>\n      <td>-0.093882</td>\n      <td>0.686083</td>\n      <td>0.406636</td>\n      <td>-0.159070</td>\n      <td>-0.000835</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>16079997</th>\n      <td>1.882130</td>\n      <td>0.001741</td>\n      <td>0.268808</td>\n      <td>-0.048209</td>\n      <td>0.412520</td>\n      <td>-0.093882</td>\n      <td>0.686083</td>\n      <td>0.406636</td>\n      <td>-0.159070</td>\n      <td>0.004686</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>16079998</th>\n      <td>1.931664</td>\n      <td>0.001741</td>\n      <td>0.268808</td>\n      <td>-0.048209</td>\n      <td>0.412520</td>\n      <td>-0.093882</td>\n      <td>0.686083</td>\n      <td>0.406636</td>\n      <td>-0.159070</td>\n      <td>0.004686</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>16079999</th>\n      <td>1.981197</td>\n      <td>-0.004050</td>\n      <td>0.268808</td>\n      <td>-0.048209</td>\n      <td>0.277329</td>\n      <td>-0.093882</td>\n      <td>0.006677</td>\n      <td>0.297740</td>\n      <td>-0.162016</td>\n      <td>-0.001111</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>16080000 rows × 33 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"  \ncategorical_vars = [col for col in X_train_encoded.columns if col not in quant_vars]\n\n# Convert quantitative variables to float (if not already)\nX_train_encoded[quant_vars] = X_train_encoded[quant_vars].astype(float)\n\n# Convert categorical variables to int\nX_train_encoded[categorical_vars] = X_train_encoded[categorical_vars].astype(int)\n\n# Convert the DataFrame to a NumPy array\nX_train_np = X_train_encoded.to_numpy()\n\nX_train_np = X_train_encoded.to_numpy()\n\n# Assuming each sequence is 100 observations long\nnum_sequences = int(X_train_np.shape[0] / 100)\nnum_features = X_train_np.shape[1]  # Number of features after encoding\n\nX_train_reshaped = X_train_np.reshape((num_sequences, 100, num_features))\n\n ","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Path to your y_train CSV file\ny_train_path = '/kaggle/input/cfmens/y_train_or6m3Ta.csv'\n\n# Read y_train using Pandas\ny_train_df = pd.read_csv(y_train_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T15:02:51.919909Z","iopub.execute_input":"2024-05-11T15:02:51.920682Z","iopub.status.idle":"2024-05-11T15:02:51.997527Z","shell.execute_reply.started":"2024-05-11T15:02:51.920628Z","shell.execute_reply":"2024-05-11T15:02:51.996740Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"y_train_df.iloc[:, -1]","metadata":{"execution":{"iopub.status.busy":"2024-05-11T14:18:25.702027Z","iopub.execute_input":"2024-05-11T14:18:25.702726Z","iopub.status.idle":"2024-05-11T14:18:25.712787Z","shell.execute_reply.started":"2024-05-11T14:18:25.702687Z","shell.execute_reply":"2024-05-11T14:18:25.711875Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"0         10\n1         15\n2          0\n3         13\n4          0\n          ..\n160795    13\n160796     1\n160797     3\n160798    11\n160799     5\nName: eqt_code_cat, Length: 160800, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":" \ny_train = y_train_df.iloc[:, -1].values  # This extracts the labels as a NumPy array\n\n \nnum_sequences = int(X_train_reshaped.shape[0])\n\nassert len(y_train) == num_sequences, \"The length of y_train does not match the number of sequences in X_train.\"\n\n \n\nnum_classes = np.unique(y_train).size\n\n# Convert labels to one-hot encoding\ny_train_categorical = to_categorical(y_train, num_classes=num_classes)\n\n# y_train_categorical is now ready to be used with your model.\n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T15:02:58.550458Z","iopub.execute_input":"2024-05-11T15:02:58.551308Z","iopub.status.idle":"2024-05-11T15:03:14.923221Z","shell.execute_reply.started":"2024-05-11T15:02:58.551266Z","shell.execute_reply":"2024-05-11T15:03:14.922167Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"2024-05-11 15:03:01.895443: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-11 15:03:01.895650: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-11 15:03:02.162362: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\n# Define the model\nmodel = Sequential([\n    # Add a Bidirectional GRU layer\n    Bidirectional(GRU(64, return_sequences=True), input_shape=(100, X_train_reshaped.shape[2])),\n    Dropout(0.1),  # Dropout for regularization\n    Bidirectional(GRU(32)),\n    Dropout(0.1),  # Another Dropout layer for regularization\n    # Output layer, assuming `num_classes` is defined from y_train preparation\n    Dense(num_classes, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Model summary\nmodel.summary()\n \n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T15:54:50.431880Z","iopub.execute_input":"2024-05-11T15:54:50.433082Z","iopub.status.idle":"2024-05-11T15:54:50.565004Z","shell.execute_reply.started":"2024-05-11T15:54:50.433041Z","shell.execute_reply":"2024-05-11T15:54:50.564033Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_2\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ bidirectional_4 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m38,016\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_5 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m31,104\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │         \u001b[38;5;34m1,560\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ bidirectional_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">38,016</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">31,104</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,560</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m70,680\u001b[0m (276.09 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">70,680</span> (276.09 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m70,680\u001b[0m (276.09 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">70,680</span> (276.09 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"\nearly_stopping = EarlyStopping(\n    monitor='val_loss',     # Monitor validation loss\n    patience=3,             # Number of epochs with no improvement after which training will be stopped\n    verbose=1,              # Output a message when stopping\n    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n)\nhistory = model.fit(\n    X_train_reshaped, \n    y_train_categorical, \n    epochs=50,  # Number of epochs: adjust based on your data and training progress\n    batch_size=64,  # Batch size: adjust based on your data and available memory\n    validation_split=0.25,\n    callbacks=[early_stopping]# Use 20% of the data for validation\n    \n)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T15:03:28.252530Z","iopub.execute_input":"2024-05-11T15:03:28.253315Z","iopub.status.idle":"2024-05-11T15:22:17.926561Z","shell.execute_reply.started":"2024-05-11T15:03:28.253277Z","shell.execute_reply":"2024-05-11T15:22:17.925658Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Epoch 1/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 19ms/step - accuracy: 0.1497 - loss: 2.6497 - val_accuracy: 0.2813 - val_loss: 2.2053\nEpoch 2/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 18ms/step - accuracy: 0.2909 - loss: 2.1643 - val_accuracy: 0.3459 - val_loss: 1.9930\nEpoch 3/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 18ms/step - accuracy: 0.3504 - loss: 1.9716 - val_accuracy: 0.3783 - val_loss: 1.8737\nEpoch 4/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 18ms/step - accuracy: 0.3893 - loss: 1.8431 - val_accuracy: 0.4158 - val_loss: 1.7596\nEpoch 5/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 18ms/step - accuracy: 0.4222 - loss: 1.7350 - val_accuracy: 0.4399 - val_loss: 1.6852\nEpoch 6/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18ms/step - accuracy: 0.4488 - loss: 1.6451 - val_accuracy: 0.4719 - val_loss: 1.5842\nEpoch 7/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18ms/step - accuracy: 0.4714 - loss: 1.5775 - val_accuracy: 0.4755 - val_loss: 1.5668\nEpoch 8/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18ms/step - accuracy: 0.4886 - loss: 1.5154 - val_accuracy: 0.4883 - val_loss: 1.5158\nEpoch 9/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 18ms/step - accuracy: 0.5036 - loss: 1.4656 - val_accuracy: 0.5097 - val_loss: 1.4486\nEpoch 10/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 18ms/step - accuracy: 0.5178 - loss: 1.4147 - val_accuracy: 0.5152 - val_loss: 1.4347\nEpoch 11/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18ms/step - accuracy: 0.5312 - loss: 1.3769 - val_accuracy: 0.5231 - val_loss: 1.4059\nEpoch 12/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18ms/step - accuracy: 0.5408 - loss: 1.3480 - val_accuracy: 0.5300 - val_loss: 1.3852\nEpoch 13/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 18ms/step - accuracy: 0.5499 - loss: 1.3216 - val_accuracy: 0.5330 - val_loss: 1.3763\nEpoch 14/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 18ms/step - accuracy: 0.5601 - loss: 1.2887 - val_accuracy: 0.5490 - val_loss: 1.3259\nEpoch 15/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 18ms/step - accuracy: 0.5678 - loss: 1.2609 - val_accuracy: 0.5500 - val_loss: 1.3289\nEpoch 16/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 18ms/step - accuracy: 0.5728 - loss: 1.2425 - val_accuracy: 0.5565 - val_loss: 1.3023\nEpoch 17/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 18ms/step - accuracy: 0.5857 - loss: 1.2108 - val_accuracy: 0.5644 - val_loss: 1.2763\nEpoch 18/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 18ms/step - accuracy: 0.5947 - loss: 1.1760 - val_accuracy: 0.5682 - val_loss: 1.2673\nEpoch 19/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18ms/step - accuracy: 0.6003 - loss: 1.1677 - val_accuracy: 0.5738 - val_loss: 1.2488\nEpoch 20/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 18ms/step - accuracy: 0.6067 - loss: 1.1468 - val_accuracy: 0.5738 - val_loss: 1.2564\nEpoch 21/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18ms/step - accuracy: 0.6149 - loss: 1.1206 - val_accuracy: 0.5822 - val_loss: 1.2322\nEpoch 22/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18ms/step - accuracy: 0.6197 - loss: 1.1057 - val_accuracy: 0.5815 - val_loss: 1.2359\nEpoch 23/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 18ms/step - accuracy: 0.6253 - loss: 1.0845 - val_accuracy: 0.5865 - val_loss: 1.2195\nEpoch 24/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 18ms/step - accuracy: 0.6270 - loss: 1.0744 - val_accuracy: 0.5958 - val_loss: 1.1902\nEpoch 25/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18ms/step - accuracy: 0.6386 - loss: 1.0513 - val_accuracy: 0.5882 - val_loss: 1.2239\nEpoch 26/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18ms/step - accuracy: 0.6408 - loss: 1.0378 - val_accuracy: 0.5991 - val_loss: 1.1849\nEpoch 27/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 18ms/step - accuracy: 0.6455 - loss: 1.0259 - val_accuracy: 0.6004 - val_loss: 1.1784\nEpoch 28/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18ms/step - accuracy: 0.6504 - loss: 1.0137 - val_accuracy: 0.6030 - val_loss: 1.1754\nEpoch 29/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18ms/step - accuracy: 0.6544 - loss: 1.0013 - val_accuracy: 0.6049 - val_loss: 1.1663\nEpoch 30/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18ms/step - accuracy: 0.6570 - loss: 0.9835 - val_accuracy: 0.6028 - val_loss: 1.1800\nEpoch 31/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 18ms/step - accuracy: 0.6594 - loss: 0.9732 - val_accuracy: 0.6090 - val_loss: 1.1750\nEpoch 32/50\n\u001b[1m1885/1885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 18ms/step - accuracy: 0.6644 - loss: 0.9660 - val_accuracy: 0.6069 - val_loss: 1.1732\nEpoch 32: early stopping\nRestoring model weights from the end of the best epoch: 29.\n","output_type":"stream"}]},{"cell_type":"code","source":"X_test = pl.scan_csv('/kaggle/input/cfmens/X_test_m4HAPAP.csv')\ndef add_feature_engineering(df):\n    # Imbalance Metrics\n    df = df.with_columns([\n        (pl.col('bid_size') - pl.col('ask_size')).alias('imbalance'),\n        (pl.col('bid_size') / (pl.col('ask_size') + 0.01)).alias('imbalance_ratio')  # Avoid division by zero\n    ])\n    \n    # Recent Price Change\n    # Calculate price change within each observation id assuming each group 'obs_id' is sorted by time\n    df = df.with_columns(\n    (pl.col('price').diff().fill_null(0).cumsum().over('obs_id')).alias('cumulative_price_change')\n)\n    \n\n    # VWAP (Volume-Weighted Average Price)\n    # VWAP is calculated as the sum of (price * volume) divided by the total volume\n    price_times_volume = pl.col('price') * pl.col('bid_size')  # Use bid_size as a proxy for volume\n    total_volume = pl.col('bid_size')\n    \n    df = df.with_columns(\n        ((price_times_volume) / (total_volume)).alias('vwap')\n    )\n\n    return df\n\n# Apply feature engineering\ndf_test = add_feature_engineering(X_test)\norder_counts = df_test.groupby(['obs_id', 'order_id']).agg(pl.count().alias('count'))\ndf_test = df_test.join(order_counts, on=['obs_id', 'order_id'])\n\ncolumns_to_shift = ['action', 'venue', 'side', 'bid', 'ask', 'bid_size', 'ask_size', 'trade']\nfor col in columns_to_shift:\n    df_test = df_test.with_columns(\n        pl.when(pl.col('count') > 1)\n        .then(pl.col(col).shift(1).over(['obs_id', 'order_id']))\n        .otherwise(pl.lit(None))\n        .alias(f'prev_{col}')\n    )\n\n# Step 3: Optionally, remove the 'count' column if no longer needed\ndf_test = df_test.drop('count')\ncategorical_columns = ['venue', 'action', 'side','trade', 'prev_action', 'prev_venue', 'prev_trade']\nfor col in categorical_columns:\n    df = df_test.with_columns(pl.col(col).cast(pl.Utf8).cast(pl.Categorical).alias(col))\nX_test_p = df.collect().to_pandas()\nX_test_encoded = pd.get_dummies(X_test_p, columns=[\n    'venue', 'action', 'side', 'trade', \n    'prev_action', 'prev_venue', 'prev_trade', 'prev_side'\n], drop_first=True).drop(['obs_id'], axis=1)\n\n \nquant_vars = [\n \n 'order_id',\n 'price',\n 'bid',\n 'ask',\n 'bid_size',\n 'ask_size',\n 'flux',\n 'imbalance',\n 'imbalance_ratio',\n 'cumulative_price_change',\n 'vwap',\n 'prev_bid',\n 'prev_ask',\n 'prev_bid_size',\n 'prev_ask_size',\n ]\n\n# transform the quantitative variables\nX_test_encoded[quant_vars] = scaler.transform(X_test_encoded[quant_vars])\ncategorical_vars = [col for col in X_test_encoded.columns if col not in quant_vars]\nX_test_encoded[quant_vars] = X_test_encoded[quant_vars].astype(float)\nX_test_encoded[categorical_vars] = X_test_encoded[categorical_vars].astype(int)\nX_test_np = X_test_encoded.to_numpy()\nnum_sequences = int(X_test_np.shape[0] / 100)\nnum_features = X_test_np.shape[1]  # Number of features after encoding\nX_test_reshaped = X_test_np.reshape((num_sequences, 100, num_features))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T15:25:50.835465Z","iopub.execute_input":"2024-05-11T15:25:50.836227Z","iopub.status.idle":"2024-05-11T15:27:53.553049Z","shell.execute_reply.started":"2024-05-11T15:25:50.836190Z","shell.execute_reply":"2024-05-11T15:27:53.551906Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_2213/464705739.py:13: DeprecationWarning: `cumsum` is deprecated. It has been renamed to `cum_sum`.\n  (pl.col('price').diff().fill_null(0).cumsum().over('obs_id')).alias('cumulative_price_change')\n/tmp/ipykernel_2213/464705739.py:30: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n  order_counts = df_test.groupby(['obs_id', 'order_id']).agg(pl.count().alias('count'))\n/tmp/ipykernel_2213/464705739.py:30: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n  order_counts = df_test.groupby(['obs_id', 'order_id']).agg(pl.count().alias('count'))\n","output_type":"stream"}]},{"cell_type":"code","source":"X_test_reshaped.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-11T15:28:20.010192Z","iopub.execute_input":"2024-05-11T15:28:20.011260Z","iopub.status.idle":"2024-05-11T15:28:20.017110Z","shell.execute_reply.started":"2024-05-11T15:28:20.011210Z","shell.execute_reply":"2024-05-11T15:28:20.016120Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"(81600, 100, 33)"},"metadata":{}}]},{"cell_type":"code","source":"# Make predictions\npredictions = model.predict(X_test_reshaped)\n\n# If your model outputs one-hot encoded predictions, convert these to label predictions\n# Assuming your model does classification\npredicted_labels = predictions.argmax(axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T15:51:19.141049Z","iopub.execute_input":"2024-05-11T15:51:19.141841Z","iopub.status.idle":"2024-05-11T15:51:34.418962Z","shell.execute_reply.started":"2024-05-11T15:51:19.141803Z","shell.execute_reply":"2024-05-11T15:51:34.417985Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"\u001b[1m2550/2550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# Example: Save predictions to CSV for submission\nsubmission_df = pd.DataFrame(predicted_labels, columns=['eqt_code_cat'])\nsubmission_df.to_csv('submission_laset.csv')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T15:52:34.416805Z","iopub.execute_input":"2024-05-11T15:52:34.417171Z","iopub.status.idle":"2024-05-11T15:52:34.515089Z","shell.execute_reply.started":"2024-05-11T15:52:34.417144Z","shell.execute_reply":"2024-05-11T15:52:34.514208Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"submission_df.eqt_code_cat.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-05-11T15:52:16.941062Z","iopub.execute_input":"2024-05-11T15:52:16.941835Z","iopub.status.idle":"2024-05-11T15:52:16.961694Z","shell.execute_reply.started":"2024-05-11T15:52:16.941796Z","shell.execute_reply":"2024-05-11T15:52:16.960569Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"eqt_code_cat\n9     64276\n14    11285\n11     2313\n0      1181\n1       925\n19      884\n12      674\n10       33\n13       10\n8         8\n3         6\n17        3\n21        1\n16        1\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"X_train_encoded","metadata":{"execution":{"iopub.status.busy":"2024-04-03T19:42:57.070594Z","iopub.execute_input":"2024-04-03T19:42:57.071577Z","iopub.status.idle":"2024-04-03T19:42:57.098232Z","shell.execute_reply.started":"2024-04-03T19:42:57.071533Z","shell.execute_reply":"2024-04-03T19:42:57.097141Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"          order_id  price   bid   ask  bid_size  ask_size  flux  venue_4  \\\n0                0   0.30  0.00  0.01       100         1   100     True   \n1                1  -0.17  0.00  0.01       100         1   100     True   \n2                2   0.28  0.00  0.01       100         1  -100     True   \n3                3   0.30  0.00  0.01       100         1   100     True   \n4                4   0.37  0.00  0.01       100         1  -100     True   \n...            ...    ...   ...   ...       ...       ...   ...      ...   \n16079995        61   1.32  0.01  0.06       735       261  -100     True   \n16079996        70   0.06  0.01  0.06       735       361   100    False   \n16079997        71   1.26  0.01  0.06       735       361   100     True   \n16079998        72   1.26  0.01  0.06       735       361   100     True   \n16079999        73   0.00  0.01  0.06       635       361     1     True   \n\n          venue_1  venue_5  venue_2  venue_0  venue_3  action_A  action_D  \\\n0           False    False    False    False    False      True     False   \n1           False    False    False    False    False      True     False   \n2           False    False    False    False    False     False      True   \n3           False    False    False    False    False      True     False   \n4           False    False    False    False    False     False      True   \n...           ...      ...      ...      ...      ...       ...       ...   \n16079995    False    False    False    False    False     False      True   \n16079996    False    False    False     True    False      True     False   \n16079997    False    False    False    False    False      True     False   \n16079998    False    False    False    False    False      True     False   \n16079999    False    False    False    False    False      True     False   \n\n          action_U  side_A  side_B  trade_false  trade_true  \n0            False    True   False         True       False  \n1            False   False    True         True       False  \n2            False    True   False         True       False  \n3            False    True   False         True       False  \n4            False    True   False         True       False  \n...            ...     ...     ...          ...         ...  \n16079995     False    True   False         True       False  \n16079996     False    True   False         True       False  \n16079997     False    True   False         True       False  \n16079998     False    True   False         True       False  \n16079999     False   False    True         True       False  \n\n[16080000 rows x 20 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>order_id</th>\n      <th>price</th>\n      <th>bid</th>\n      <th>ask</th>\n      <th>bid_size</th>\n      <th>ask_size</th>\n      <th>flux</th>\n      <th>venue_4</th>\n      <th>venue_1</th>\n      <th>venue_5</th>\n      <th>venue_2</th>\n      <th>venue_0</th>\n      <th>venue_3</th>\n      <th>action_A</th>\n      <th>action_D</th>\n      <th>action_U</th>\n      <th>side_A</th>\n      <th>side_B</th>\n      <th>trade_false</th>\n      <th>trade_true</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.30</td>\n      <td>0.00</td>\n      <td>0.01</td>\n      <td>100</td>\n      <td>1</td>\n      <td>100</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>-0.17</td>\n      <td>0.00</td>\n      <td>0.01</td>\n      <td>100</td>\n      <td>1</td>\n      <td>100</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.28</td>\n      <td>0.00</td>\n      <td>0.01</td>\n      <td>100</td>\n      <td>1</td>\n      <td>-100</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.30</td>\n      <td>0.00</td>\n      <td>0.01</td>\n      <td>100</td>\n      <td>1</td>\n      <td>100</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.37</td>\n      <td>0.00</td>\n      <td>0.01</td>\n      <td>100</td>\n      <td>1</td>\n      <td>-100</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16079995</th>\n      <td>61</td>\n      <td>1.32</td>\n      <td>0.01</td>\n      <td>0.06</td>\n      <td>735</td>\n      <td>261</td>\n      <td>-100</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>16079996</th>\n      <td>70</td>\n      <td>0.06</td>\n      <td>0.01</td>\n      <td>0.06</td>\n      <td>735</td>\n      <td>361</td>\n      <td>100</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>16079997</th>\n      <td>71</td>\n      <td>1.26</td>\n      <td>0.01</td>\n      <td>0.06</td>\n      <td>735</td>\n      <td>361</td>\n      <td>100</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>16079998</th>\n      <td>72</td>\n      <td>1.26</td>\n      <td>0.01</td>\n      <td>0.06</td>\n      <td>735</td>\n      <td>361</td>\n      <td>100</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>16079999</th>\n      <td>73</td>\n      <td>0.00</td>\n      <td>0.01</td>\n      <td>0.06</td>\n      <td>635</td>\n      <td>361</td>\n      <td>1</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>16080000 rows × 20 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Use a CNN-BIGRU\n","metadata":{}},{"cell_type":"code","source":"\n\n# Define the model\nmodel = Sequential([\n    # Add a 1D Convolutional layer to extract features\n    Conv1D(filters=64, kernel_size=3, activation='selu', input_shape=(100, X_train_reshaped.shape[2])),\n    MaxPooling1D(pool_size=2),\n    Dropout(0.2),  # Dropout for regularization\n\n    # Add a Bidirectional GRU layer\n    Bidirectional(GRU(64, return_sequences=True)),\n    Dropout(0.2),  # Dropout for regularization\n    Bidirectional(GRU(32)),\n    Dropout(0.2),  # Another Dropout layer for regularization\n\n    # Output layer, assuming `num_classes` is defined from y_train preparation\n    Dense(num_classes, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Model summary\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T15:30:09.117830Z","iopub.execute_input":"2024-05-11T15:30:09.118556Z","iopub.status.idle":"2024-05-11T15:30:09.268883Z","shell.execute_reply.started":"2024-05-11T15:30:09.118517Z","shell.execute_reply":"2024-05-11T15:30:09.267937Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m6,400\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m49,920\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m31,104\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │         \u001b[38;5;34m1,560\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,920</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">31,104</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,560</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m88,984\u001b[0m (347.59 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">88,984</span> (347.59 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m88,984\u001b[0m (347.59 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">88,984</span> (347.59 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(\n    monitor='val_loss',     # Monitor validation loss\n    patience=3,             # Number of epochs with no improvement after which training will be stopped\n    verbose=1,              # Output a message when stopping\n    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T15:30:14.360135Z","iopub.execute_input":"2024-05-11T15:30:14.361100Z","iopub.status.idle":"2024-05-11T15:30:14.366110Z","shell.execute_reply.started":"2024-05-11T15:30:14.361050Z","shell.execute_reply":"2024-05-11T15:30:14.364935Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    X_train_reshaped, \n    y_train_categorical, \n    epochs=50,\n    batch_size=64,\n    validation_split=0.2,\n    callbacks=[early_stopping]  # Include the early stopping callback\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T15:30:17.329995Z","iopub.execute_input":"2024-05-11T15:30:17.330720Z","iopub.status.idle":"2024-05-11T15:50:37.340103Z","shell.execute_reply.started":"2024-05-11T15:30:17.330682Z","shell.execute_reply":"2024-05-11T15:50:37.339014Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Epoch 1/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 14ms/step - accuracy: 0.1644 - loss: 2.6030 - val_accuracy: 0.3068 - val_loss: 2.1198\nEpoch 2/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.3147 - loss: 2.0949 - val_accuracy: 0.3778 - val_loss: 1.8845\nEpoch 3/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.3685 - loss: 1.9071 - val_accuracy: 0.4192 - val_loss: 1.7472\nEpoch 4/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.4114 - loss: 1.7720 - val_accuracy: 0.4413 - val_loss: 1.6726\nEpoch 5/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.4444 - loss: 1.6611 - val_accuracy: 0.4815 - val_loss: 1.5484\nEpoch 6/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.4632 - loss: 1.5915 - val_accuracy: 0.4919 - val_loss: 1.5071\nEpoch 7/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.4834 - loss: 1.5282 - val_accuracy: 0.4943 - val_loss: 1.4970\nEpoch 8/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.4995 - loss: 1.4737 - val_accuracy: 0.5139 - val_loss: 1.4281\nEpoch 9/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.5105 - loss: 1.4416 - val_accuracy: 0.5308 - val_loss: 1.3849\nEpoch 10/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.5197 - loss: 1.4073 - val_accuracy: 0.5401 - val_loss: 1.3631\nEpoch 11/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.5316 - loss: 1.3784 - val_accuracy: 0.5426 - val_loss: 1.3478\nEpoch 12/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.5414 - loss: 1.3484 - val_accuracy: 0.5539 - val_loss: 1.3168\nEpoch 13/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.5519 - loss: 1.3205 - val_accuracy: 0.5593 - val_loss: 1.2952\nEpoch 14/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.5565 - loss: 1.3012 - val_accuracy: 0.5676 - val_loss: 1.2675\nEpoch 15/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.5612 - loss: 1.2813 - val_accuracy: 0.5717 - val_loss: 1.2550\nEpoch 16/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.5720 - loss: 1.2497 - val_accuracy: 0.5778 - val_loss: 1.2316\nEpoch 17/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.5741 - loss: 1.2391 - val_accuracy: 0.5858 - val_loss: 1.2092\nEpoch 18/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.5787 - loss: 1.2315 - val_accuracy: 0.5897 - val_loss: 1.2067\nEpoch 19/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.5852 - loss: 1.2064 - val_accuracy: 0.5956 - val_loss: 1.1849\nEpoch 20/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.5911 - loss: 1.1938 - val_accuracy: 0.5971 - val_loss: 1.1828\nEpoch 21/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.5957 - loss: 1.1797 - val_accuracy: 0.6004 - val_loss: 1.1662\nEpoch 22/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.5989 - loss: 1.1653 - val_accuracy: 0.6004 - val_loss: 1.1736\nEpoch 23/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.6034 - loss: 1.1602 - val_accuracy: 0.6058 - val_loss: 1.1558\nEpoch 24/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.6049 - loss: 1.1498 - val_accuracy: 0.6046 - val_loss: 1.1533\nEpoch 25/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.6098 - loss: 1.1399 - val_accuracy: 0.6095 - val_loss: 1.1424\nEpoch 26/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.6138 - loss: 1.1237 - val_accuracy: 0.6075 - val_loss: 1.1581\nEpoch 27/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.6156 - loss: 1.1153 - val_accuracy: 0.6119 - val_loss: 1.1460\nEpoch 28/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.6210 - loss: 1.1047 - val_accuracy: 0.6139 - val_loss: 1.1331\nEpoch 29/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.6235 - loss: 1.0994 - val_accuracy: 0.6121 - val_loss: 1.1387\nEpoch 30/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.6246 - loss: 1.0918 - val_accuracy: 0.6222 - val_loss: 1.1039\nEpoch 31/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.6274 - loss: 1.0832 - val_accuracy: 0.6267 - val_loss: 1.0974\nEpoch 32/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.6286 - loss: 1.0813 - val_accuracy: 0.6211 - val_loss: 1.1091\nEpoch 33/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.6314 - loss: 1.0730 - val_accuracy: 0.6212 - val_loss: 1.1045\nEpoch 34/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.6342 - loss: 1.0622 - val_accuracy: 0.6288 - val_loss: 1.0965\nEpoch 35/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.6376 - loss: 1.0559 - val_accuracy: 0.6244 - val_loss: 1.1047\nEpoch 36/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.6388 - loss: 1.0520 - val_accuracy: 0.6303 - val_loss: 1.0829\nEpoch 37/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.6382 - loss: 1.0517 - val_accuracy: 0.6233 - val_loss: 1.1023\nEpoch 38/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.6428 - loss: 1.0382 - val_accuracy: 0.6366 - val_loss: 1.0765\nEpoch 39/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.6428 - loss: 1.0376 - val_accuracy: 0.6322 - val_loss: 1.0847\nEpoch 40/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.6482 - loss: 1.0215 - val_accuracy: 0.6417 - val_loss: 1.0505\nEpoch 43/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.6509 - loss: 1.0143 - val_accuracy: 0.6388 - val_loss: 1.0675\nEpoch 44/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.6505 - loss: 1.0120 - val_accuracy: 0.6416 - val_loss: 1.0534\nEpoch 45/50\n\u001b[1m2010/2010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.6543 - loss: 1.0018 - val_accuracy: 0.6308 - val_loss: 1.0805\nEpoch 45: early stopping\nRestoring model weights from the end of the best epoch: 42.\n","output_type":"stream"}]}]}